% !TEX enableShellEscape = yes
% (The above line makes atom's latex package compile with -shell-escape
% for minted, and is just ignored by other systems.)
\documentclass{article}

\usepackage{fullpage}
\usepackage{color}
\usepackage{amsmath,amssymb}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{parskip}
\usepackage{amssymb}
\usepackage{hyperref}

% Use one or the other of these for displaying code.
% NOTE: If you get
%  ! Package minted Error: You must invoke LaTeX with the -shell-escape flag.
% and don't want to use minted, just comment out the next line
\usepackage{minted} \BeforeBeginEnvironment{minted}{\begingroup\color{black}} \AfterEndEnvironment{minted}{\endgroup} \setminted{autogobble,breaklines,breakanywhere,linenos}

\usepackage{listings}

% Colours
\definecolor{blu}{rgb}{0,0,1}
\newcommand{\blu}[1]{{\textcolor{blu}{#1}}}
\definecolor{gre}{rgb}{0,.5,0}
\newcommand{\gre}[1]{\textcolor{gre}{#1}}
\definecolor{red}{rgb}{1,0,0}
\newcommand{\red}[1]{\textcolor{red}{#1}}
\definecolor{pointscolour}{rgb}{0.6,0.3,0}

% answer commands
\newcommand\ans[1]{\par\gre{Answer: #1}}
\newenvironment{answer}{\par\begingroup\color{gre}Answer: }{\endgroup}
\let\ask\blu
\let\update\red
\newenvironment{asking}{\begingroup\color{blu}}{\endgroup}
\newcommand\pts[1]{\textcolor{pointscolour}{[#1~points]}}

% Math
\def\R{\mathbb{R}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand{\norm}[1]{\lVert #1 \rVert}
\newcommand{\mat}[1]{\begin{bmatrix}#1\end{bmatrix}}

% LaTeX
\newcommand{\fig}[2]{\includegraphics[width=#1\textwidth]{#2}}
\newcommand{\centerfig}[2]{\begin{center}\includegraphics[width=#1\textwidth]{#2}\end{center}}

\begin{document}


\title{CPSC 340 Assignment 6 (due Wednesday April 12 at 11:55pm)}
\date{}
\maketitle
\vspace{-6em}

\blu{Name(s) and Student ID(s):}
\ans{\\
Dean Yang 67057695\\
Xuan Tung Luu 30236798
}

\section{Robust PCA for Background Subtraction}

If you run \verb|python main.py 1|, the program will load a dataset $X$ where each row contains the pixels from a single frame of a video of a highway. The demo applies PCA to this dataset and then uses this to reconstruct the original image.
It then shows the following 3 images for each frame:
\begin{enumerate}
	\item The original frame.
	\item The reconstruction based on PCA.
	\item A binary image showing locations where the reconstruction error is non-trivial.
\end{enumerate}
Recently, latent-factor models have been proposed as a strategy for ``background subtraction'': trying to separate objects from their background. In this case, the background is the highway and the objects are the cars on the highway.

Why does this make any sense? Remember that PCA tries to find principal components that reconstruct the dataset well. Here the training examples are highway images. Now, we need to reconstruct these highway images out of a small number of PC images. Since we want to reconstruct them well, we're going to create PCs that capture the \emph{common features across the entire dataset}. In other words, the PCs will try to reconstruct the highway and slight variations of the highway, because that is always there. If a car appears in the top-left of only a couple images, we might not want a PC for that case, since that PC is useless for the majority of the training set. Thus, when we try to reconstruct the images we are left to see only the parts of the image that can be made out of the PCs, or in other words that are common across all the images. Hence why the reconstructions look like the background. We can then subtract this reconstruction (background) from the original image to get objects of interest (foreground). Cool, right?

In this demo, we see that PCA does an OK job of identifying the cars on the highway in that it does tend to identify the locations of cars. However, the results aren't great as it identifies quite a few irrelevant parts of the image as objects. We can use robust PCA to do even better.

Robust PCA is a variation on PCA where we replace the L2-norm with the L1-norm,
\[
f(Z,W) = \sum_{i=1}^n\sum_{j=1}^d |\langle w^j, z_i\rangle - x_{ij}|,
\]
and it has recently been proposed as a more effective model for background subtraction.

\subsection{Why Robust PCA}

\ask{In a few sentences, explain why using a robust loss might help PCA perform better for this background subtraction application.} Some conversation-starters: what is an outlier here --- a car or the highway? What does it mean to be robust to outliers --- that we're good at reconstructing them or that we're bad at reconstructing them?
\ans{A car is an outlier. It performs better because it is more robust against outliers, which means that robust PCA is better at reconstructing the highway without cars (bad reconstruction of cars). Hence it can better separate the highway and the cars when we do the subtraction}

\subsection{Robust PCA Approximation and Gradient}

If you run \verb| python main.py 1.3 |, the program will repeat the same procedure as in the above section, but will attempt to use the robust PCA method, whose objective functions are yet to be implemented. You will need to implement it.

We'll use a gradient-based approach to PCA and a smooth approximation to the L1-norm. In this case the log-sum-exp approximation to the absolute value may be hard to get working due to numerical issues. Perhaps the Huber loss would work. We'll use the ``multi-quadric'' approximation:
\[
|\alpha| \approx \sqrt{\alpha^2 + \epsilon},
\]
where $\epsilon$ controls the accuracy of the approximation (a typical value of $\epsilon$ is $0.0001$). Note that when $\epsilon=0$ we revert back to the absolute value, but when $\epsilon>0$ the function becomes smooth.

Our smoothed loss is:

\[
f(Z,W) = \sum_{i=1}^n\sum_{j=1}^d \sqrt{(\langle w^j, z_i\rangle - x_{ij})^2 + \epsilon }
\]

The partial derivatives of this loss with respect to the elements of $W$ are:

\begin{align*}
\frac{\partial}{\partial w_{cj}} f(Z,W)
  &= \frac{\partial}{\partial w_{cj}} \sum_{i=1}^n\sum_{j'=1}^d \left( (\langle w^{j'}, z_i\rangle - x_{ij'})^2 + \epsilon \right)^{\frac12} \\
  &= \sum_{i=1}^n \frac{\partial}{\partial w_{cj}} \left( (\langle w^{j}, z_i\rangle - x_{ij})^2 + \epsilon \right)^{\frac12}  \qquad \text{(since the $j' \ne j$ terms have no $w_{cj}$ in them)} \\
  &= \sum_{i=1}^n \frac12 \left( (\langle w^{j}, z_i\rangle - x_{ij})^2 + \epsilon \right)^{-\frac12} \frac{\partial}{\partial w_{cj}} \left( (\langle w^{j}, z_i\rangle - x_{ij})^2 + \epsilon \right) \\
  &= \sum_{i=1}^n \frac12  \left( (\langle w^{j}, z_i\rangle - x_{ij})^2 + \epsilon \right)^{-\frac12} \;2\,  (\langle w^{j}, z_i\rangle - x_{ij}) \; \frac{\partial}{\partial w_{cj}} \langle w^{j}, z_i\rangle \\
  &= \sum_{i=1}^n \left( (\langle w^{j}, z_i\rangle - x_{ij})^2 + \epsilon \right)^{-\frac12}  (\langle w^j, z_i\rangle - x_{ij}) \, z_{ic}
\end{align*}

The partial derivatives with respect to $Z$ are similar:

\begin{align*}
\frac{\partial}{\partial z_{ic}} f(Z,W)
  &= \frac{\partial}{\partial z_{ic}} \sum_{i'=1}^n \sum_{j=1}^d  \left( (\langle w^j, z_{i'}\rangle - x_{i'j})^2 + \epsilon \right)^{\frac12}\\
  &= \sum_{j=1}^d  \left( (\langle w^j, z_{i}\rangle - x_{ij})^2 + \epsilon \right)^{-\frac12}   (\langle w^j, z_{i}\rangle - x_{ij}) \; \frac{\partial}{\partial z_{ic}} \langle w^j, z_i\rangle \\
  &= \sum_{j=1}^d  \left( (\langle w^j, z_i\rangle - x_{ij})^2 + \epsilon \right)^{-\frac12}  (\langle w^j, z_i\rangle - x_{ij}) \, w_{cj}
\end{align*}

If we put this into matrix(ish) notation, we get the following:

\[
\nabla_W f(Z,W) = Z^T \left[ R \oslash \left(R^{\circ 2} + \epsilon\right)^{\circ \frac12}  \right]
\]

where $R\equiv ZW-X$,
$A \oslash B$ denotes \textbf{element-wise} division of $A$ and $B$,
$A + s$ for a scalar $s$ denotes element-wise adding $s$ to each entry of $A$,
and $A^{\circ p}$ denotes taking $A$ to the \textbf{element-wise} power of $p$.

And, similarly, the gradient with respect to $Z$ is given by:

\[
\nabla_Z f(Z,W) = \left[ R \oslash \left(R^{\circ 2} + \epsilon\right)^{\circ \frac12} \right] W^T
\]

\ask{Show that the two parts of the gradient above, $\nabla_W f(Z,W)$ and $\nabla_Z f(Z,W)$, have the expected dimensions.}
\ans{\\
Z is an $n\times k$ matrix. So $Z^T$ is an $k\times n$ matrix. (1)\\ 
W is a $k \times d$ matrix. So $W^T$ is a $d \times k$ matrix. (2)\\
X is an $n \times d$ matrix.\\
$R\equiv ZW-X$ so R should have the same dimensions as X (element-wise subtraction). Therefore, R is an $n \times d$ matrix
\\\\
Each term of the gradients should have the same dimensions as the variable it is taking gradient with respect to. 
\\\\
The gradient with respect to W, 
$\nabla_W f(Z,W) = Z^T \left[ R \oslash \left(R^{\circ 2} + \epsilon\right)^{\circ \frac12}  \right]$
, should hence have the dimensions $k\times d$
\\\\
The gradient with respect to Z, $\nabla_Z f(Z,W) = \left[ R \oslash \left(R^{\circ 2} + \epsilon\right)^{\circ \frac12} \right] W^T$, should hence have the dimensions $n\times k$
\\\\
$\left[ R \oslash \left(R^{\circ 2} + \epsilon\right)^{\circ \frac12} \right]$ denotes an element-wise division of R and the second term so this whole thing should have the same dimensions as R, which is $n \times d$ (3).
\\\\ 
(2)(3) so 
$\nabla_Z f(Z,W) = \left[ R \oslash \left(R^{\circ 2} + \epsilon\right)^{\circ \frac12} \right] W^T$ had the dimensions of $(n \times d) \times (d \times k) = n \times k$, which matches that of $Z$
\\\\
(1)(3) so
$\nabla_W f(Z,W) = Z^T \left[ R \oslash \left(R^{\circ 2} + \epsilon\right)^{\circ \frac12}  \right]$ had the dimensions of $(k \times n) \times (n \times d) = k \times d$, which matches that of $W$
}

\subsection{Robust PCA Implementation}

In \texttt{fun\_obj.py}, you will find classes named \texttt{RobustPCAFactorsLoss} and \texttt{RobustPCAFeaturesLoss} which should compute the gradients with respect to $W$ and $Z$, respectively. \ask{Complete the \texttt{evaluate()} method for each using the smooth approximation and gradient given above. Submit (1) your code in \texttt{fun\_obj.py}, and (2) one of the resulting figures.}

Hint: Your code will look similar to the already implemented \texttt{PCAFactorsLoss} and \texttt{PCAFeaturesLoss} classes. Note that the arguments for \texttt{evaluate()} are carefully ordered and shaped to be compatible with our optimizers.

Note: The robust PCA is somewhat sensitive to initialization, so multiple runs might be required to get a reasonable result.
\centerfig{0.7}{figs/q1.3-code}
\centerfig{0.6}{figs/robustpca_highway_000}

\subsection{Reflection}
\ask{Answer the following questions and briefly justify your answer.}
\begin{enumerate}
	\item Very briefly comment on the results from the previous section --- does robust PCA seem to do better than regular PCA for this task?
 \ans{Robust PCA seems to be better at separating entire cars, it generates more solid shapes and less error.}
	\item How does the number of video frames and the size of each frame relate to $n$, $d$, and/or $k$?
 \ans{$n$ is the total number of video frames, $d$ is the total number of pixels in each image, equals width $\times$ height. Width and height are indicators of the size of the image.}
	\item What would the effect be of changing the threshold (see code) in terms of false positives (cars we identify that aren't really there) and false negatives (real cars that we fail to identify)?
 \ans{If the threshold is lower, the algorithm would generate more false positives (it detects more subtle differences). If the threshold is higher, it will generate more false negatives (it is less sensitive to small differences).}
\end{enumerate}

\section{Neural Networks}

\subsection{Neural Networks by Hand}

Suppose that we train a neural network with sigmoid activations and one hidden layer and obtain the following parameters (assume that we don't use any bias variables):
\[
W = \mat{-2 & 2 & -1\\1 & -2 & 0}, v = \mat{3 \\1}.
\]
\ask{Assuming that we are doing regression, for a training example with features $x_i^T = \mat{-4 &-2 & 4}$ what are the values in this network of $z_i$, $h(z_i)$, and $\hat{y}_i$? (show your work)}
\ans{\\
$z_i = Wx_i = \mat{-2 & 2 & -1\\1 & -2 & 0}\mat{-4 \\-2 \\ 4} = \mat{0 \\ 0}$ 
\\
$h(z_i) = h(Wx_i) = \mat{h(0) \\ h(0)} = \mat{1/2 \\ 1/2}$
\\
$\hat{y}_i = v^Th(Wx_i) = \mat{3 & 1}\mat{1/2 \\ 1/2} = 2$
}

\subsection{Neural Networks vs. Linear Classifier}

If you run \texttt{python main.py 2}, the program will train a multi-class logistic regression classifier on the MNIST handwritten digits data set using stochastic gradient descent with some pre-tuned hyperparameters.
The scikit-learn implementation called by the code uses a minibatch size of 1 and a one-vs-rest strategy for multi-class logistic regression. It is set to train for 10 epochs.
The performance of this model is already quite good, with around 8\% training and validation errors. Your task is to use a neural networks classifier to outperform this baseline model.

If you run \texttt{python main.py 2.2}, the program will train a single-layer neural network (one-layer nonlinear encoder paired with a linear classifier) on the same dataset using some pre-tuned hyperparameters. Modify the code, play around with the hyperparameter values (configurations of encoder layers, batch size and learning rate of SGD, standardization, etc.) until you achieve validation error that is reliably below \red{5}\%. \ask{Submit your modified code and report (1) the training and validation errors that you obtain with your hyperparameters, and (2) the hyperparameters that you used.}

\centerfig{0.8}{figs/q2.2-code}
\ans{\\
hidden feature dims = 60, 60\\
output dim = k\\
learning rate multiplier = 4e-3\\
batch size = 1000\\
max evals=20\\
used standardized X\\
}



\subsection{Neural Disentangling}

An intuitive way to think about a neural network is to think of it as a composition of an encoder (all the layers except the last one, including the final nonlinearity) and a linear predictor (the last layer). The predictor signals the encoder to learn a better feature space, in such a way that the final linear predictions are meaningful.

The following figures resulted from running \texttt{python main.py 2.3}. For this, a neural network model is used to encode the examples into a 2-dimensional feature space. Here's the original training data:

\centerfig{.7}{./figs/sinusoids.png}

And here is the learned decision boundary of the neural network:

\centerfig{.7}{./figs/sinusoids_decision_boundary_[2]_2.png}

We can look inside and see what the encoder learned. Here is the training data shown in the 2D transformed feature space learned by the encoder (first layer):

\centerfig{.7}{./figs/sinusoids_learned_features_[2]_2.png}

Here is the linear classifier learned by the predictor in this transformed feature space:

\centerfig{.7}{./figs/sinusoids_linear_boundary_[2]_2.png}


This particular neural network solution does not classify the given examples very well. However, if we use a 4-dimensional hidden features (with all else equal), we get a better-looking result:

\centerfig{.7}{./figs/sinusoids_decision_boundary_[4]_2.png}
\centerfig{.7}{./figs/sinusoids_linear_boundary_[4]_2.png}

Note: in this case, since the hidden feature space has dimension 4, we are only able to plot 2 of the 4 dimensions. The actual linear classifiers is a 3D hyperplane dividing the 4D space. However, even with these two dimensions that we can plot, we can already see that the training data looks pretty much linearly separable, and we learn a good classifier.

Previously we changed the \emph{hidden layer size} from 2 to 4. Another change we can make is to increase the \emph{number of layers} from 1 to 2. Let's use 2 layers of hidden features of size 3:

\centerfig{.7}{./figs/sinusoids_decision_boundary_[3, 3]_2.png}
\centerfig{.7}{./figs/sinusoids_linear_boundary_[3, 3]_2.png}

\ask{Answer the following questions and provide a brief explanation for each:}

\begin{enumerate}
	\item Why are the axes ($z_1$ and $z_2$) of the learned feature space scatterplot constrained to $[0, 1]$? Note/hint: here, our definition of $z$ is after the nonlinearity is applied, whereas in the lecture $z$ is defined as before the nonlinearity is applied. Sorry about that!
	%
 \ans{Transforming the weighted sum using the sigmoid function constrains the values of z to (0,1)}
	\item Does it make sense to use neural networks on datasets whose examples are linearly separable in the original feature space?
	%
  \ans{Using neural networks for linearly separable data means that no feature transformations are needed. While neural networks will be able to classify correctly without problems, it may be more efficient to use linear regression models.}
	\item Why would increasing the dimensionality of the hidden features (which is a hyper-parameter) help improve the performance of our classifier?
	%
\ans{With more dimensions, a layer will capture more diverse combinations of the previous layer.}
	\item Why would increasing the number of layers help improve the performance of our classifier?
	%
 \ans{Because each layer captures combinations of parts of the previous layer so more layers can capture more complex features.}
	\item Neural networks are known to suffer problems due to a highly non-convex objective function. What is one way we can address this problem and increase the chances of discovering a global optimum? (Hint: look at the code.)
	%
 \ans{Doing random restarts a number of times.}
\end{enumerate}


\section{Very-Short Answer Questions}

\ask{Answer each of the following questions in a sentence or two.}
\begin{enumerate}

\item Give one reason why you might want sparse solutions with a latent-factor model.
\ans{Dimensionality reduction allows for cheaper computation.}
\item{Are neural networks mainly used for supervised or unsupervised learning? Are they parametric or non-parametric?}
\ans{They are mainly used for supervised learning. They are parametric as the number of weights are fixed regardless of the size of dataset.}
\item{Why might regularization become more important as we add layers to a neural network?}
\ans{The complexity of the neural network increases as more layers are added, which means it is more likely to overfit to the training data. Regularization reduces overfitting.}
\item{Consider using a fully-connected neural network for 5-class classification on a problem with $d=10$. If the network has one hidden layer of size $k=100$, how many parameters (including biases), does the network have?}
\ans{$11*100+101*5=1605$}
\item Consider fitting a linear latent-factor model, using L1-regularization of the $w_c$ values and L2-regularization of the $z_i$ values,
\[
f(Z,W) = \frac{1}{2}\norm{ZW - X}_F^2 + \lambda_W \sum_{c=1}^k \left[\norm{w_c}_1\right] + \frac{\lambda_Z}{2} \sum_{i=1}^n \left[\norm{z_i}^2\right],
\]
\begin{enumerate}
\item What is the effect of $\lambda_Z$ on the two parts of the fundamental trade-off in machine learning? 
\ans{Higher $\lambda_Z$ leads to lower approximation error but higher training error}
\item What is the effect of $k$ on the two parts?
\ans{Lower k leads to lower approximation error but higher training error}
\item Would either of answers to the \emph{previous two questions} change if $\lambda_W = 0$?
\ans{Yes, if $\lambda_W = 0$ then regularization has no effect on the fundamental trade-off. Therefore, changing $\lambda_Z$  and k will have no effect on the fundamental trade-off.}
\end{enumerate}
\item Which is better for recommending movies to a new user, collaborative filtering or content-based filtering? Briefly justify your answer.
\ans{Collaborative filtering cannot be used as new users do not have any ratings. So content-based filtering is the only valid option.}
\item Consider fitting a neural network with one hidden layer. Would we call this a parametric or a non-parametric model if the hidden layer had $n$ units (where $n$ is the number of training examples)? Would it be parametric or non-parametric if it had $d^2$ units (where $d$ is the number of input features).
\ans{\\
If the hidden layer had $n$ units, it is a non-parametric model since the size of the model increase on the number of examples.\\
If it had $d^2$ units, it is a parametric model since the size of the model will stay the same on the number of examples.}
\item Consider a neural network with $L$ hidden layers, where each layer has at most $k$ hidden units and we have $c$ labels in the final layer (and the number of features is $d$). What is the cost of prediction with this network?
\ans{$O(dk+k^2L+kc)$}
\item What is the cost of backpropagation in the previous question?
\ans{$O(dk+Lk^2)$}
\item What is the ``vanishing gradient'' problem with neural networks based on sigmoid non-linearities?
\ans{Sigmoid function has a gradient of nearly zero away from the origin. The problem is worse when taking sigmoid of a sigmoid. Therefore, in deep neural nets, taking gradient will always lead to 0 and SGD does not move.}
\item Convolutional networks seem like a pain... why not just use regular (``fully-connected'') neural networks for image classification?
\ans{Because summarizing local neighbors can give a better context than just one pixel alone.}

\end{enumerate}
\end{document}
